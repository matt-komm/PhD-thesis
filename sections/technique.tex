\chapter{Analysis techniques}

\intro{This chapter introduces the employed toolset for the measurements within this thesis. It is organized to follow the common steps of the analysis strategies. First, the generation of simulated events is introduced. Then, details of the top quark reconstruction, the employed multivariate analysis technique, and the template-based \acrlong{ml} fit is given. These allow for a precise estimation of the amount of single top quark events within data. To compare with theoretical predictions, objects at parton and particle level are defined to which the observed data shape is unfolded. Lastly, the unfolding problem is analyzed and a regularized unfolding technique is introduced.}

%##############################################
\section{Event generation}
%##############################################
\label{sec:technique-event-gen}

To compare reconstructed data with theoretical predictions, samples of simulated events are generated from theory and passed through a simulation of the \gls{cms} detector and an emulation of its readout. The standard so-called ``FullSimulation'' package~\cite{1742-6596-396-2-022003,1742-6596-664-7-072022} is based on the Geant4 toolkit~\cite{Agostinelli2003250} which provides a detailed simulation of particle trajectories and interactions with the detector material. A fast alternative, the so-called ``FastSimulation'' package, exists within \gls{cms} as well~\cite{fsimRahmat} but it has not been used for the analyses within this thesis.

The generation of events begins with the \glshere{me} of a hard scattering process of interest. \glshere{mc} methods are employed to sample the corresponding cross section integral. The advantage of \gls{mc}-based methods is that the variance of their result decreases as $1/n$ independently of the integral's dimensionality, where $n$ is the number of samples. This makes \gls{mc} methods particular efficient for calculating cross sections compared to quadrature-based methods~(e.g. Simpson's rule, Newton-Cotes). A common method to integrate cross sections is given by the \gls{vegas} algorithm~\cite{OHL199913}. It is based on importance sampling where the integral is sampled not uniformly but along an adaptive density function instead. The resulting sample of events reflects the probability distribution of a process over its final state phase space. A reweighting is typically performed in addition such that all events contribute the same probability i.e., they carry the same absolute weight. 

After obtaining events from the hard interaction, a \glshere{ps} program simulates the hadronization of final state partons. In addition, radiation of soft gluons or quarks from initial or final state partons is simulated as well which is referred to as \glshere{isr} or \glshere{fsr} respectively. Furthermore, contributions from soft secondary interactions, the so-called underlying event, and potential color reconnection effects are taken into account. A sketch of an exemplary pp collision event after hadronization is shown in Fig.~\ref{fig:technique-mcevent} where various parts of the event simulation are highlighted. 

Parton shower simulations are based on Altarelli-Parisi splitting functions~\cite{Altarelli:1977zs} which allow to calculate the probability of soft parton splitting, e.g. $\mathrm{q}\to \mathrm{gq}$, $\mathrm{g}\to \mathrm{q}\bar{\mathrm{q}}$. It is convenient to calculate the ``surviving'' probability, the so-called Sudakov factor, that a parton does not branch further below a certain energy scale. During the \gls{ps} simulation a complication arises from potential double counting of soft parton emissions since the upstream simulation of the hard interaction may have led already to a corresponding soft emission. These are avoided by applying a dedicated \gls{me}-to-\gls{ps} matching scheme which consists of a method to assign additional emissions exclusively to either the simulation of the hard interaction or the \gls{ps} depending on the event kinematics. More information on parton shower simulation and matching algorithms can be found in Refs.~\cite{Hoche:2014rga,Alwall:2007fs}.


\myfigure{\label{fig:technique-mcevent} A sketch of a generated event from the simulation of the hard interaction and subsequent hadronization through a parton shower. The figure is taken in parts from Ref.~\cite{Hoche:2014rga}.}{
\includegraphics[scale=0.75]{figures/technique/shower.pdf}
}

A brief overview of the employed programs used for event generation and subsequent hadronization for $t$-channel single top quark production is given in the following.

\begin{description}
\item[MadGraph5\_aMC{@}NLO] The \MGAMC[] program~\cite{Alwall:2014hca} is a merge of the \gls{lo} \MG[] generator~\cite{Alwall:2011uj} and \AMC[] into a common framework. It supports the generation of \gls{lo} or \gls{nlo} samples which can be matched to parton showers using the \gls[]{mlm}~\cite{Mangano:2006rw} or MC{@}NLO~\cite{Frixione:2002ik} schemes respectively. The latter method produces a certain fraction of events with negative weights (depending on the process) which stem from a subtraction of additional emissions from the \gls{nlo} matrix element to prevent double-counting. 
Multiple samples of events with additional final state partons at matrix element level can also be merged into a combined sample. Here, the overlap with the \gls{ps} is removed through the \gls[]{fxfx} merging scheme~\cite{Frederix:2012ps}.

\item[Powheg] The \POWHEG[] box (versions~1,2)~\cite{Alioli:2010xd} is a program that contains predefined implementations of various processes such as $t$-channel single top quark production~\cite{Alioli:2009je} at \gls{nlo}. It utilizes the so-called \POWHEG method~\cite{Frixione:2007vw} for matching in which the hardest radiation generated from the \gls{me} has priority over subsequent \gls{ps} emissions. This removes the overlap with the \gls{ps} without the generating negatively weighted events.

\item[CompHEP] The \COMPHEP[] program (version~4.5)~\cite{Boos:2004kh} can perform calculations of cross sections from Lagrangian densities at \gls{lo}. In addition, generation of events is also possible such as single top quark production~\cite{Boos:2006af}. Here, an approximation is used by combining events from the $2\to2$ and $2\to3$ processes which reproduces \gls{nlo} corrections in an effective way.

\item[Tauola] Event generators can be interfaced with the \TAUOLA[] library~\cite{Jadach:1993hs,Davidson:2010rw} to simulate leptonic and hadronic decays of tau leptons. It accounts for spin polarization effects while also including the radiation of photons from \gls{qed} corrections by incorporating the \PHOTOS[] library~\cite{Golonka:2005pn}.

\item[MadSpin] The generation of processes involving resonances and their decay products is a computational-intensive task especially at \gls{nlo}. The complexity can be reduced by only simulating an event up to narrow resonances like top quarks or Higgs bosons. In the narrow width approximation, where a resonant particle is taken to be on-shell, the production and decay amplitude factorizes which allows to split the simulation into separate steps. The \MADSPIN[] program~\cite{Artoisenet:2012st} extends this approach by also accounting for off-shell effects through a partial reweighting of events. In addition, spin correlations effects between production and decay products are taken into account as well.

\item[Pythia] The \PYTHIA[] program (versions~6,8)~\cite{Sjostrand:2006za,Sjostrand:2014zea} can generate events of various processes at \gls{lo}. It is however famous for its \gls{ps} simulation which can be interfaced with other \gls{lo} and \gls{nlo} event generators to perform subsequent parton showering, hadronization, and the simulation of the underlying event. For hadronization, a phenomenological model is utilized in which one-dimensional strings\footnote{The string model is motivated by the fact that the spatial form of a dipole color field does not extend radially like an \gls{em} field but is instead squeezed to a tube-like form.} connected to partons reflect the color field leading to the creation of additional partons through string branching and finally to the formation of color-neutral singlets.

\item[Herwig++] The \HERWIG[] program (version~7)~\cite{Bellm:2015jjp} is an \gls{nlo} event generator which is also capable of simulating the showering of partons similar to \PYTHIA. Its hadronization algorithm utilizes a model in which color-connected quarks are spatially kept together in clusters~\cite{Webber:1983if} which is motivated by the ``preconfinement'' concept for colored particles~\cite{Amati:1979fg}. If the mass of a cluster is sufficiently high it can decay into lighter clusters with a certain probability. In its final step, a cluster decays then into hadrons.
\end{description}


%##############################################
\section{Top quark reconstruction}
%##############################################
\label{sec:technique-topreco}

In the presented analyses, a top quark candidate is reconstructed in data and simulated events under the assumption of $t$-channel signal top quark production after the reconstruction and selection of analysis objects. Its energy and momentum is reconstructed by summing the 4-momenta of a selected lepton candidate (muon or electron), a b-tagged jet, and a neutrino candidate while assuming that the top quark decayed leptonically as $\mathrm{t}\to\mathrm{b}\mathrm{W}\to\mathrm{b}\ell\nu$. The neutrino candidate itself is reconstructed from the missing transverse energy and the lepton momentum. This is performed by requiring a W~boson mass constraint of $m_\mathrm{W}=80.4~\GeV$~\cite{Olive:2016xmw} on their invariant mass as

\begin{align}
m_\mathrm{W}^2=\colvec{2}{E_\mathrm{W}}{\vec{p}_\mathrm{W}}^{2}&\overset{!}{=}\left[\colvec{2}{E_{\ell}}{\vec{p}_{\ell}}+\colvec{2}{E_{\nu}}{\vec{p}_{\nu}}\right]^{2}\nonumber\\
&=\underbrace{m_{\ell}^2+m_{\nu}^2}_{\approx 0}+\,2\cdot E_{\ell}\,E_{\nu}-\,2\cdot\colvec{3}{p_{\ell,x}}{p_{\ell,y}}{p_{\ell,z}}\cdot\colvec{3}{p_{\nu,\mathrm{T}}\cdot\cos\phi_{\nu}}{p_{\nu,\mathrm{T}}\cdot\sin\phi_{\nu}}{p_{\nu,z}}\,, \label{eq:technique-neutrino-pz-eq}
\end{align}

where the lepton and neutrino are approximated to be massless. Then, one can solve for the unknown $p_{\nu,z}$-component of the neutrino candidate momentum by taking $p_{\nu,\mathrm{T}}$ and $\phi_{\nu}$ from the missing transverse momentum vector $\pvmiss$. After rearranging, the quadratic equation 

\begin{subequations}
\begin{align}
0&=p_{\nu,z}^2-\frac{2\,\xi\,p_{\ell,z}}{E_{\ell}^{2}-p_{\ell,z}^2}\cdot p_{\nu,z}-\frac{\xi^{2}-E_{\ell}^{2}\,p_{\nu,\mathrm{T}}^2}{E_{\ell}^{2}-p_{\ell,z}^2}\\
\xi&=\frac{m_\mathrm{W}^2}{2}+p_{\nu,\mathrm{T}}\,p_{\ell,\mathrm{T}}\cdot\cos\big(\phi_\ell-\phi_\nu\big)
\end{align}
\end{subequations}

is obtained from Eq.~\ref{eq:technique-neutrino-pz-eq} which possesses the solutions

\begin{align}
p_{\nu,z}^{1,2}=\frac{1}{E_{\ell}^{2}-p_{\ell,z}^{2}}\left[\xi\cdot p_{\ell,z}\pm E_{\ell} \sqrt{\xi^2-p_{\nu,\mathrm{T}}^2\big(E_{\ell}^2-p_{\ell,z}^2\big)}~\right]. \label{eq:technique-neutrino-pz}
\end{align}

A detailed derivation and discussion of this result can be found in Ref.~\cite{Chwalek:1416031}. In simulated $t$-channel single-top-quark events this procedure leads to two real solutions in about 65\% of all events that pass the selection. The difference of the two real solutions with respect to the true neutrino $p_{z}$ at parton level is shown in Fig.~\ref{fig:technique-neutrino-reco}. The plot demonstrates that choosing the solution which has the smallest absolute $|p_{\nu,z}|$ yields on average a neutrino $p_{z}$ which is closest to the true neutrino $p_{z}$ amongst the two real solutions.

\myfigure{\label{fig:technique-neutrino-reco}Differences of the reconstructed neutrino $p_{z}$ with respect to the neutrino $p_{z}$ at parton level in cases of real and complex solutions for simulated events of $t$-channel single-top-quark production.}{
\includegraphics[width=0.52\textwidth]{figures/technique/neutrino_match_dpz.pdf}
}

Complex solutions are obtained if the discriminant in Eq.~\ref{eq:technique-neutrino-pz} becomes negative. This happens in about 35\% for selected signal events. Such solutions occur mostly due to the finite \met resolution, whereas the negligence of the W~boson mass width and the resolution of the lepton momentum are found to be minor effects. The imaginary part of the solutions is removed by requiring that the discriminant vanishes

\begin{equation}
0\overset{!}{=}\xi^2-p_{\nu,\mathrm{T}}^2\big(E_{\ell}^2-p_{\ell,z}^2\big)\quad
\Rightarrow~ p_{\nu,z}=\frac{\xi\cdot p_{\ell,z}}{E_{\ell}^{2}-p_{\ell,z}^{2}}\,,\label{eq:technique-pz-complex}
\end{equation}

which is equivalent to setting the transverse mass $m_\mathrm{T}$ to the W~boson mass itself as

\begin{align}
m_\mathrm{W}^2\overset{!}{=}m_\mathrm{T}^2&=(p_{\ell,\mathrm{T}}+p_{\nu,\mathrm{T}})^2-(p_{\ell,x}+p_{\nu,x})^{2}-(p_{\ell,y}+p_{\nu,y})^{2}\nonumber\\
&\approx2\,p_{\ell,\mathrm{T}}^2\,p_{\nu,\mathrm{T}}^2\cdot\Big(1-\cos\big(\phi_\ell-\phi_\nu\big)\Big)\,.
\end{align}

Then, the $p_{\nu,x}$ and $p_{\nu,x}$ components are varied and the pair which minimizes the distance $|\vec{p}_{\nu,\mathrm{T}}-\pvmiss|$ with respect to the measured missing transverse momentum vector is taken as the result. Figure~\ref{fig:technique-neutrino-reco} shows that after removing the imaginary part~(Eq.~\ref{eq:technique-pz-complex}) the $p_{\nu,z}$ solution is on average closer to the true neutrino $p_{\nu,z}$ momentum compared to cases with real solutions. This can be explained as follows. Initially $m_\mathrm{T}>m_\mathrm{W}$ is found in cases of complex solutions which is then modified to $m_\mathrm{T}=m_\mathrm{W}$ by ignoring the imaginary part and applying the minimization procedure as described above. One can therefore argue that this represents a correction of the finite \met resolution which resulted into the complex solutions in the first place.

After finding a solution for the unknown neutrino $p_{z}$ component, a top quark candidate can be constructed. A comparison of the shapes of the reconstructed top quark mass and pseudorapidity for the two neutrino solution cases are presented in Fig~\ref{fig:technique-top-reco} for simulated signal events. The reconstruction yields a top quark candidate with an improved mass resolution for events with initial complex solutions compared to events with real solutions. Similarly, the pseudorapidity of the top quark candidate demonstrates an improved reproduction of the corresponding observable at parton level as well.

\myfigure{\label{fig:technique-top-reco} Shape differences in the reconstructed top quark (a)~mass and (b)~pseudorapidity for cases with real neutrino $p_{z}$ solutions  (where the one with the smallest $|p_{z}|$ is picked) or with initially complex solutions for simulated events of $t$-channel single-top-quark production.}{
\subfloat[]{\includegraphics[width=0.48\textwidth]{figures/technique/top_match_mass.pdf}}\hspace{0.03\textwidth}
\subfloat[]{\includegraphics[width=0.48\textwidth]{figures/technique/top_match_eta.pdf}}
}


%##############################################
\section{Boosted Decision Trees}
%##############################################

After the event selection, the $t$-channel single-top-quark signal region, defined by one isolated lepton, two jets (where one is b-tagged), and significant \met, is found to be still largely contaminated by events from background processes in data. The ratios of the \glshere{sb} yields amount to about 13\% and 14\% in measurements at 8~and 13~TeV, respectively~\cite{Khachatryan:2014iya,Sirunyan:2016cdg}\todo{update TOP-16-003 when accepted by PRL}. The majority of background events stems from \wjets, \ttbar, and multijet production whereas contributions from single top quark tW and $s$-channel, Drell-Yan, and diboson production are minor. The small \gls{sb} ratios motivate the usages \glshere{mva} techniques. In this thesis, \glsplhere{bdt} are employed for event classification as implemented in the \TMVA[] framework~\cite{Hocker:2007ht}. They are based on a set of decision trees where each yields a binary output depending on whether an event is signal- or background-like. Their training and how the outputs of single decision trees are combined into a one-dimensional discriminant are detailed in the following.

An exemplary decision tree is presented in Fig.~\ref{fig:technique-decisiontree}. It consists of sequential selections on observables $x_{i}$ which are applied on events such that the leaf nodes contain either a majority of signal or background contributions. Such trees are constructed by using samples of simulated events for which the desired classification is a priori known~(``superivsed learning'').

\myfigure{\label{fig:technique-decisiontree} A sketch of a decision tree.}{
\includegraphics[scale=0.75]{figures/technique/decision_tree.pdf}
}

The optimal observable $x_{i}$ and working point $C_{i}$  are found per node by maximizing the separation between the signal and background distributions. In the analyses, the separation is measured as the cross entropy 

\begin{align}
H&=-p\cdot\ln(p)-(1-p)\cdot\ln(1-p)\\ p_{i}&=\frac{\int_{C_{i}}^{\infty} N_\mathrm{sig.}(x_{i})\,\mathrm{d}x_{i}}{\int_{C_{i}}^{\infty} N_\mathrm{sig.}(x_{i})+N_\mathrm{bkg.}(x_{i})\,\mathrm{d}x_{i}}\\
\Rightarrow &~\{x_{i},\,C_{i}\}=\max\Big(H\,\big|~x_{i},\,C_{i}\Big)\,,
\end{align}

where $p$ denotes the achieved purity of a selection $x_{i}>C_{i}$. Other common measures of separation in literature are the misclassification error or the so-called ``Gini'' index~\cite{Gini}. The measures are constructed to be symmetric when swapping the signal and background classes since obtaining a background leaf with high purity is of equal importance. A node is not split further if it contains less than a predefined minimum number of events to ensure that the decisions per node and the binary outputs per leaf are statistically significant. This also mitigates a potential ``overtraining'' of a decision tree which occurs when statistical fluctuations are learned instead of the underlying physical distributions due to the finite statistics of the training sample. Additional caution is required when using a sample which contains a portion of negatively weighted events~(e.g. generated with \MGAMC). In such a case, a tree may be trained incorrectly if a large fraction of negatively weighted events are selected in one of the nodes since the distributions of observables can contain regions with unphysical yields. To achieve the cancellation of negatively weighted events per node and leaf, the minimum number of events can be increased further beyond the statistical motivated threshold, one would choose when training only on a sample of purely positively weighted events. In addition, the working point values per observable which are analyzed to find the optimal node splitting can be preset which prevents that a decision becomes sensitive to single events close to the selection border.

Single decision trees can still be perceptible to statistical fluctuation leading to misclassification errors when given a statistically-independent test sample. This is mitigated by training multiply decision trees with binary outputs $h_{i}\in\{-1,1\}$ that are then combined into a pseudo-continuous discriminant using a majority vote

\begin{equation}
M(\vec{x})=\sum_{i}^{N_\mathrm{trees}}~w_{i}\cdot h_{i}\,(\vec{x};\,\vec{C}_{i})\,.\label{eq:technique-majority-vote}
\end{equation}

Here, each decision tree output is multiplied by a so-called boosting weight $w_{i}$. This procedure has another advantage. It has been demonstrated in literature that the majority vote can yield a classifier with high accuracy by applying a ``boosting'' procedure~\cite{Schapire1990,FREUND1995256}. This means that a ``strong learner'' can already be obtained if the individual decision trees are just ``weak learners'' i.e., they have only a low classification accuracy that is slightly better than random guessing. Decision trees which are combined by a majority vote can therefore be kept very shallow, e.g. only two or three layers of selections. This also improves their robustness against overtraining. By adjusting the weights in the majority vote according to the individual accuracy per tree, a strong learner is obtained. Here, a boosting procedure determines the decision trees and the corresponding weights.

\todo{write better}

The boosting procedures employed in this thesis are the adaptive boosting~(\ADABOOST[])~\cite{FREUND1997119} and the \GRADIENTBOOST[]~\cite{Friedman00greedyfunction} algorithms. In the \ADABOOST algorithm, decision trees are trained iteratively. At each step, a single decision tree is trained and the misclassified events are identified. Their weight is then increased in the training of subsequent trees by the boosting weight

\begin{equation}
\alpha_{n+1}=\left(\frac{1-\epsilon_{n}}{\epsilon_{n}}\right)^\beta
\end{equation}

where $\epsilon_{i}$ denotes the misclassification rate of the current tree $n$ and $\beta$ is a configurable learning rate. The corresponding weight in Eq.~\ref{eq:technique-majority-vote} is then given by $w_{i}=\ln\alpha_{i}$. Typically, a slow learning rate of $\beta\leq0.5$ is chosen to allow for more boosting steps. It can be shown that the \ADABOOST algorithm is equivalent to the minimization of the exponential loss function $L(M,y)=\exp(-M(\vec{x})\,y)$ where $y$ denotes the true classification of events~\cite{Hocker:2007ht}. If the loss function is instead changed to 

\begin{equation}
L(M,y)=\ln\left(1+e^{-2\,M(\vec{x})\,y}\right)
\end{equation}

the \GRADIENTBOOST algorithm is obtained. Its loss function is more robust in the presence of outliers and noise events for which the \ADABOOST algorithm may degrade. The algorithm commences by iteratively minimizing the loss function with respect to the weights and decision tree parameters in $M$ using the method of gradient decent. During the minimization steps, the output of the majority vote will gradually tend towards $y$ because misclassified events result in large gradients of the loss function. Similar to the \ADABOOST algorithm, an increased performance is obtained when decreasing the learning rate controlling the boosting weights which is called ``shrinkage'' here. In this thesis, both boosting algorithms have been tested to validate their performances with respect to each other. Only negligible differences in their discrimination power have been found after optimizing their training parameters.

The discrimination power of a trained \gls{bdt} is assessed by analyzing the \glshere{roc} curve. Exemplary \gls{roc} curves are presented in Fig.~\ref{fig:technique-bdt-rocs}. The \gls[]{auc} values denote the area under the \gls{roc} curve with respect to random guessing. Here, the \gls{roc} curves of a \gls{bdt} trained to discriminate $t$-channel single-top-quark events against \wjets and \ttbar background events is compared to the pseudorapidity of the untagged light jet and to the reconstructed top quark mass difference. An \gls{auc} of about $32\%$ is achieved with the trained \gls{bdt} which outperforms the other two typical event observables for separating $t$-channel events from background processes. The exact setup of the \gls{bdt} shown here will be discussed in Sec.~\ref{sec:diff13-bdt} together with the corresponding analysis.

\myfigure{\label{fig:technique-bdt-rocs}Comparison of \gls{roc} curves for separating $t$-channel single-top-quark events from background events (\wjets, \ttbar) using: random guessing; a trained \gls{bdt} discriminant; the pseudorapidity of the untagged light jet ($\jprime$); the difference between the reconstructed top quark mass and the nominal \gls{mc} mass, $|\Delta m_\mathrm{top}|=|m_\mathrm{top}^\scriptn{\mathrm{reco.}}-172.5~\GeV|$.}{
\includegraphics[width=0.5\textwidth]{figures/technique/rocs.pdf}
}


%##############################################
\section{Template-based fitting}
%##############################################

The estimation of the amount of signal and background events in data is performed with template-based \glshere{ml} fits. For an observable to be fitted, histograms of simulated events act as templates which reflect the expected distributions of events per process. One can express the likelihood that the observed distribution in data is a realization of the expectation from simulation as

\begin{equation}
\mathsf{L}_\mathrm{Poi.}=\prod_{i}^\mathrm{bins}~\frac{p_{i}^{\,d_{i}}\cdot e^{-p_{i}}}{d_{i}!},\qquad p_{i}=\beta^{\mathrm{(sig.)}}\cdot t^{\mathrm{(sig.)}}_{i}+\sum_{j}^\mathrm{bkgs.}~\beta^{(j)}\cdot t^{(j)}_{i} \,, \label{eq:technique-likelihood}
\end{equation}

\todo{template fits profit from discriminating distributions with different s/b ratios}

where the amount of observed data events $d_{i}$ per bin $i$ is modeled as a Poisson distribution with the expected event yield $p_{i}$ as mean. The expected yields per bin are obtained by summing the signal and background templates $t^\scriptn{(X)}_{i}$. The normalization of the templates can be modified through scale factors $\beta^\scriptn{\mathrm{(X)}}$ which are estimated from data by maximizing the likelihood. The signal scale factor is also referred to as signal ``strength'' whereas the background scale factors are sometimes called nuisance parameters because their result is of less importance. Technically, the \THETA[] framework~\cite{theta} is employed for template-based fitting where the negative logarithm of the likelihood

\begin{equation}
-\ln\Big(\mathsf{L}_\mathrm{Poi.}\big(\vec{\beta}\big)\Big)=-\sum_{i}^\mathrm{bins}~\Big[\,d_{i}\ln p_{i}\big(\vec{\beta}\big)-p_{i}\big(\vec{\beta}\big)\Big]+\mathrm{const.}
\end{equation}

is minimized for numerical stability. An estimated scale factor can be directly translated into a corresponding cross section since the templates are normalized to their corresponding \gls{sm} cross sections multiplied with the integrated luminosity of data. One obtains

\begin{align}
\hat{\sigma}_\mathrm{sig.}&=\frac{\hat{N}_\mathrm{sig.}}{A\cdot\epsilon\cdot{\textstyle{\int}L}}~,\quad
\hat{N}_\mathrm{sig.}=\hat{\beta}_\mathrm{sig.}\cdot N_\mathrm{exp.}=\hat{\beta}_\mathrm{sig.}\cdot\underbrace{\sigma_\mathrm{\gls{sm}}\cdot\textstyle{\int}L}_\mathrm{norm.}\cdot \overbrace{A \cdot \epsilon}^\mathrm{sel./reco.}\nonumber\\
&=\hat{\beta}_\mathrm{sig.}\cdot\sigma_\mathrm{\gls{sm}}\,, \label{eq:technique-xsec-measurement}
\end{align}

where $A$ denotes the acceptance and $\epsilon$ the efficiency of the event selection and reconstruction which are intrinsically estimated through the simulated samples. In the fit, additional constraints are typically applied for the background scale factors by adding log-normal priors to the likelihood as

\begin{equation}
-\ln\Big(\mathsf{L}_\mathrm{total}\Big)=-\ln\Big(\mathsf{L}_\mathrm{Poi.}\big(\vec{\beta}\big)\Big)+\sum_{j}^\mathrm{bkgs.}~\frac{1}{2}\cdot\left(\frac{\ln\,\beta^{(j)}}{\delta^{(j)}}\right)^{2}\,.
\end{equation}

The additional constraints with uncertainties $\pm\delta$ reflect a priori believes of the background contributions in the analysis phase space where the fit is carried out. They are motivated by the fact that the selected analysis phase space is usually not optimized to measure the background yields very precisely. Here, log-normal distributions are explicitly preferred over Gaussian distributions for implementing these constraints because log-normal distributions are not biased when requiring that the scale factors are always positive in cases of large uncertainties. For Gaussian distributions with large widths, such a truncation shifts the mean of the distribution which then biases the fit result.

Extra nuisance parameters are considered in the fit to account for the limited accuracy of the predicted event yields per bin due to the finite statistics of the simulated event samples. A method proposed by R. Barlow and C. Beeston~\cite{Barlow:1993dm}, commonly called \glshere{bb} method, introduces additional nuisance parameters $\nu_{ij}$ per bin $i$ and process $j$ which modify the predicted yields as $p_{i}^\prime=\sum_{j}\nu_{ij}\cdot p_{ij}$ while adding additional constraints to the likelihood reflecting the finite \gls{mc} uncertainties. This approach however increases the complexity of the fit since many more parameters have to be estimated in addition to the signal and background scale factors. The number of \gls{bb} parameters is reduced in the Barlow-Beeston lite method~\cite{Conway:2011in} where the uncertainties per bins are grouped and describe by only a single nuisance parameter. A further, technical simplification can be achieved when using numerical minimization algorithms for fitting. Here, it is computationally advantageous to only maximize the likelihood with respect to the scale factors while profiling the \gls{bb} parameters per bin in each step in-situ. This approach is also implemented in the employed \THETA framework. One should note that such an approach can result into discontinuous jumps of the likelihood. Certain minimization algorithms (e.g. \MINUIT~\cite{James:1975dr}) may therefore not converge properly since the Hessian matrix is not positive-definite near such discontinuities. Therefore, \THETA is explicitly configured to use the simple iterative Newton algorithm for minimization instead.



%##############################################
\section{Parton and particle level observables}
%##############################################

Cross sections can be measured not only inclusively but also differentially in intervals of an observable. They offer the means to perform in-deep comparisons of data with theoretical predictions. Such measurements allow to assess the modeling and validity of event generators and analytical calculations. Furthermore, as detailed in Ch.~\ref{ch:top}, special differential cross sections are sensitive to the coupling structure of a certain process and can thus be used to extract pseudo observables like the top-quark spin asymmetry which is sensitive to the polarization. However, a proper comparison of differential cross sections can only be achieved if the corresponding observable is well-defined, and identical across event generators and in analytical calculations. There are two ``levels'', parton and particle level, at which the physical objects of a process and related observables are typically defined. 

The ``parton level'' encompasses the intermediate and final state particles within Feynman diagrams of a given process while not distinguishing between different initial states. In event generators, these partonic particles are produced as the hard interaction before hadronization. An exemplary event of $t$-channel single-top-quark production at parton level is shown in Fig~\ref{fig:technique-parton-example}. 

\myfigure{\label{fig:technique-parton-example}Production and decay of an exemplary $t$-channel single-top-quark event in 4~\gls{fs} at parton level as produced by \POWHEG{v2} interfaced with \MADSPIN and \PYTHIA{8}. Some intermediate state particles and vertices are not stored by the generator. Copies of particles reflecting the exclusion or inclusion of boosts induced by \gls{qcd}/\gls{qed} radiations have been omitted.}{
\includegraphics[scale=0.75]{figures/technique/massiveRadiation.pdf}
}

Some intermediate particles and vertices are not stored by the generator for simplicity but also due to the quantum nature of a process. Since events are generated to follow the probability of the squared sum of various matrix elements contributing to a certain process, one can usually not associate a generated event to only one specific Feynman diagram with absolute certainty. Nonetheless, various particle objects and related observables can be defined at parton level since those can be unambiguously found in the event record as detailed in the following. \todo{check here}
\begin{description}
\item[Prompt leptons] Leptons which originate from decays of Z or W~bosons that are associated to the hard interaction are called ``prompt'' which are distinguished from leptons produced in hadron decays. In $t$-channel single-top-quark production, about 15\% of the prompt muons or electrons from the top quark decay stem from an intermediate tau lepton as it is for example shown in Fig.~\ref{fig:technique-parton-example} as well. This fraction depends on the \pt threshold of the muon or electron as depicted in Fig.~\ref{fig:technique-parton-muon}. It reduces to about 6\% when requiring an electron or muon with a transverse momentum of at least $20~\GeV$ in the final state.
\item[Prompt neutrino] Neutrinos are also required to be prompt and thus do not stem from hadron decays. In the case of leptonically decaying taus, a prompt ``pseudo'' neutrino is defined by summing the 4-momenta of both neutrinos occurring in the decay.
\item[Top quark] The partonic top quark is defined to be on-shell while accounting for \gls{qcd}/\gls{qed} radiations and the intrinsic \kt of the initial state partons which may boost the event in the transverse plane.
\item[Spectator quark] The spectator quark is required to be produced in association with the single top quark, i.e. they share common ancestors as depicted in Fig.~\ref{fig:technique-parton-example}. Furthermore, the spectator quark has be a light flavored quark~(u,d,s,c) to distinguish it from a potential second b~quark occurring in 4~\gls{fs} production. At \gls{nlo}, the production of additional light quarks through \gls{isr} can lead to an ambiguity here. In such cases, the quark which balances best the top quark momentum in the transverse plane is chosen.
\end{description}

\myfigure{\label{fig:technique-parton-muon}Distributions of the (a)~transverse momentum and (b)~pseudorapidity of the final state lepton produced in $t$-channel single top quark production at $13~\TeV$. The lines represent various decays of W~bosons into either muons/taus directly or via intermediate tau decays. The distributions have been generated using \POWHEG{v2} interfaced with \MADSPIN and \PYTHIA{8}.}{
\subfloat[]{\includegraphics[width=0.48\textwidth]{figures/technique/lepton_pt.pdf}}\hspace{0.03\textwidth}
\subfloat[]{\includegraphics[width=0.48\textwidth]{figures/technique/lepton_eta.pdf}}
}

The polarization angle can be calculated from the defined objects at parton level. A comparison of its shape for the various W~boson decay chains is presented in Fig.~\ref{fig:technique-parton-cosTheta}. For tau leptons, the $\cos\theta_\mu^\star$ shape is found nearly identical compared to muons or electrons despite the larger tau mass. The shape is distorted when requiring events for which the lepton momentum is above a certain threshold. This results into a drop of the distribution at $\cos\theta^{*}_{\mu}\to 1$ which is demonstrated in Fig.~\ref{fig:technique-parton-cosTheta20} where the polarization angle is shown for events with $\pt(\mu)>20~\GeV$ only.

\myfigure{\label{fig:technique-parton-cosTheta}Distributions of the polarization angle for $t$-channel single top quark production at $13~\TeV$: (a)~inclusive distribution; (b)~only events with $\pt(\mu)>20~\GeV$. The lines represent various decays of W~bosons into either muons/taus directly or via intermediate tau decays. The distributions have been generated using \POWHEG{v2} interfaced with \MADSPIN and \PYTHIA{8}.}{
\subfloat[]{\includegraphics[width=0.48\textwidth]{figures/technique/cosTheta.pdf}}\hspace{0.03\textwidth}
\subfloat[\label{fig:technique-parton-cosTheta20}]{\includegraphics[width=0.48\textwidth]{figures/technique/cosTheta_lpt20.pdf}}
}

Another level at which observables can be defined is the ``particle level''. Here, an event selection similar to the one at reconstruction level is applied on the generated particles after hadronization. This allows to report results close to the ``fiducial'' phase space of the detector. The advantage is that an extrapolation into the inclusive phase space as it is intrinsically performed for parton level measurements (e.g. inclusive cross sections) is not required. A sketch of parton and particle level objects for $t$-channel single-top-quark production is shown in Fig.~\ref{fig:technique-parton-particle}. Objects at particle level are defined by utilizing only the final and stable particles produced by event generators and subsequent parton showers with a mean lifetime of more than $30~\mathrm{ps}$. Here, final state quarks and gluons appear as jets after hadronization which consist of hadrons and non-prompt leptons. Charged leptons may radiate photons which are accounted for by clustering close-by photons with charged leptons. These are referred to as ``dressed'' leptons. The performed clustering enables a universal treatment of \gls{qcd}/\gls{qed} radiations without utilizing information about intermediate particles in the decay chain which is more complicated at parton level (especially in the definition of the spectator quark) and may be generator-dependent as well. Detailed definitions of the analysis objects at particle level used in this thesis are provided in the following.

\myfigure{\label{fig:technique-parton-particle}A sketch of parton and particle level objects in $t$-channel single-top-quark production. An additional gluon has been added as a final state radiation.}{
\includegraphics[scale=0.75]{figures/technique/fiducial.pdf}
}

\begin{description}
\item[Dressed leptons] Photons which do not stem from hadron decays are clustered with prompt muons or electrons if they are within $\Delta R=\sqrt{\Delta\eta^2+\Delta\phi^2}<0.1$. A dressed lepton consists of exactly one charged lepton and any number of potentially close-by photons.
\item[Neutrinos] The 4-momenta of all prompt neutrinos are summed to define the missing transverse energy. To improve the agreement with the neutrino candidate at reconstruction level, the same algorithm as described in Sec.~\ref{sec:technique-topreco} is applied at particle level using a dressed lepton here to solve for the neutrino $p_{z}$ component.
\item[Jets] Jet are clustered from stable particles excluding all neutrinos and the particles used to define the dressed leptons. The anti-$k_\mathrm{T}$ algorithm with a distance of $R=0.4$ is employed at 13~TeV mimicking the jet size at reconstruction level. The ``ghost'' b-tagging method~\cite{Cacciari:2008gn} where jets are tagged containing B-hadrons with rescaled momentum will not be used here since its high efficiency is found to disagree with the performance of the tagging algorithm at reconstruction level~(see Sec.~\ref{sec:diff13-fiducial-studies}).
\item[Pseudo top quark] A pseudo top quark is reconstructed by combining the 4-momenta of a dressed lepton, a neutrino candidate, and a jet. \todo{top mass jet}
\end{description}

After applying a suitable selection on the particle level objects, the fiducial cross section can obtained from the inclusive cross section by modifying Eq.~\ref{eq:technique-xsec-measurement} as

\begin{equation}
\sigma_\mathrm{sig.}^\mathrm{fid.}=\frac{N_\mathrm{sig.}\cdot A_\mathrm{fid.}}{A_\mathrm{reco.}\cdot\epsilon_\mathrm{reco.}\cdot{\textstyle{\int}L}} =\sigma_\mathrm{sig.}^\mathrm{inc.}\cdot A_\mathrm{fid.}\,,
\end{equation}

where $A_\mathrm{fid.}$ denotes the acceptance of events. Similar to the acceptance and efficiency of the event selection at reconstruction level, it can be estimated from a sample of simulated signal events as $A_\mathrm{fid.}=N_\mathrm{fid.}/N_\mathrm{total}$. 



%##############################################
\section{Unfolding}
%##############################################

In simple terms, the idea of unfolding in \gls{hep} is to ``revert'' the effects of the detector in data such as smearing, finite resolution, acceptance, and reconstruction inefficiencies. An unfolded distribution can then be easily compared to the expectations from theory and to equivalent measurements, e.g. obtained from other analysis channels or experiments.

Unfolding can be understood as follows. For a reconstructed distribution one can write

\begin{equation}
\underbrace{~f(y)~}_\mathrm{reco.}=\int \underbrace{A(y)\,\epsilon(y)\, R(y,x)}_\mathrm{detector}~~\cdot \underbrace{~g(x)~}_\mathrm{true}\, \mathrm{d}x\,, \label{eq:technique-fredholm}
\end{equation}

where a true distribution $g(x)$ (i.e. at parton or particle level) is folded with the detector response $R(y,x)$ times the acceptance of the event selection $A(y)$ and the reconstruction efficiency $\epsilon(y)$. Mathematically, Eq.~\ref{eq:technique-fredholm} is called a Fredholm equation of first kind. Unfolding is then a procedure for inferring an estimate of the true distribution given $f(y)$, $A(y)$, $\epsilon(y)$, and $R(y,x)$. 

Equation~\ref{eq:technique-fredholm} can be discretized and written as a matrix equation

\begin{equation}
\vec{y} = \widetilde{\mathcal{R}}\cdot\vec{x}\,,\qquad \widetilde{\mathcal{R}}=\mathcal{A}\cdot\mathcal{E}\cdot\mathcal{R}\,, \label{eq:technique-folding}
\end{equation}

where the continuous distributions are converted into vectors~(histograms) and the response, acceptance, and efficiency functions are described by matrices~(two-dimensional histograms). Elements of the response matrix $\mathcal{R}_{ij}$ can be interpreted as the transition probability $p_{i\to j}$ that an observable in bin $i$ at truth level is measured in bin $j$ by the detector. Attempting to solve Eq.~\ref{eq:technique-folding} for $\vec{x}$ through a simple inversion of the response matrix reveals that the unfolding problem is actually ill-posed. A simple inversion results into unstable solutions with large variances and significant anticorrelations between bins which can be observed as oscillations in $\vec{x}$~\cite{Cowan:2002in}. Figure~\ref{fig:technique-ill-unfolding} demonstrates this for a simple model which is defined as

\begin{subequations}\label{eq:technique-unfolding-test-model}
\begin{align}
g(x)&=\frac{1}{2}+A\cdot x\,,\qquad A=0.44\\
R(y,x)&\propto\mathrm{exp}\left(\frac{1}{2}\cdot\frac{(x-y)^2}{\sigma^2}\right)\,,\qquad \sigma=0.15\,.
\end{align}
\end{subequations}

Here, a distribution $g$, similar to the expected distribution of the top-quark polarization angle at parton level, has been folded with a simple Gaussian smearing function $R$. When unfolding by inversion of the response matrix, a small deviation~(dash-orange line) from the folded distribution~(violet markers) results into an unphysical, oscillating solution.

\myfigure{\label{fig:technique-ill-unfolding}Exemplary unfolding of a distribution using a simple inversion of the response matrix: (a)~true distribution with overlaid unfolding results; (b)~the true distribution after folding and a sample of a corresponding statistical fluctuation.}{
\subfloat[\label{fig:technique-ill-unfolding-true}]{\includegraphics[width=0.48\textwidth]{figures/technique/trueDist.pdf}}\hspace{0.03\textwidth}
\subfloat[\label{fig:technique-ill-unfolding-reco}]{\includegraphics[width=0.48\textwidth]{figures/technique/recoDist.pdf}}
}


This origin of these oscillations can be investigated by performing a \glshere{svd} of the unfolding problem~(Eq.~\ref{eq:technique-folding}). The \gls{svd} is a generalization of the eigendecomposition that allows to decompose even non-quadratic matrices as 

\begin{equation}
\vec{x}=\Big(\widetilde{\mathcal{R}}\Big)^{-1}\vec{y}\\
=\Big(\mathcal{U}~\cdot\underbrace{\mathcal{S}}_\mathrm{diagonal}\cdot~\mathcal{V}\Big)^{-1}\vec{y}~=~\mathcal{V}^{-1}\cdot
\begin{tikzpicture}[baseline=(current bounding box.center),every left delimiter/.style={xshift=0.4em},every right delimiter/.style={xshift=-.4em},]
\matrix (m) [matrix of math nodes,nodes in empty cells,row sep=-1.5mm,left delimiter=(,right delimiter={)},inner sep=0pt,nodes={inner sep=.3333em},]{
\frac{1}{s_{11}} &  & 0  \\
0 & & \frac{1}{s_{nn}} \\
} ;
\draw[dotted,thick] (m-1-1)-- (m-2-3);
\end{tikzpicture}
\cdot~~\mathcal{U}^{-1}~\vec{y}\,,
\end{equation}


where $\mathcal{U}$ and $\mathcal{V}$ contain the so-called left- and right-singular vectors, respectively. The matrix $\mathcal{S}$ has only non-zero elements $s_{ii}$ on its diagonal which are also referred to as singular values. Typically, they are ordered as $s_{ii}>s_{i+1,i+1}$ while the singular vectors are normalized to $1$. The vectors are orthogonal to each other and can be interpreted as modes of the measured distribution. When a singular value is small, the unfolded distribution becomes unstable since the corresponding mode in the reconstructed distribution is unphysically amplified by $1/s_{ii}$ through the inversion.

Various regularization procedures have been proposed to mitigate this problem. The most straight forward regularization scheme is utilized in the ``\gls{svd} unfolding'' algorithm~\cite{Hocker:1995kb}. The response matrix is modified as

\begin{equation}
\Big(\mathcal{R}^\mathrm{reg.}[\tau]\Big)^{-1}_{ ij}=\sum_{i}^{n}\,\left(\,\sum_{k}^{\tau}~\mathcal{V}^{-1}_{ik}\cdot\left(\frac{1}{s_{kk}}\right)\cdot~\mathcal{U}^{-1}_{kj}\right)\,,
\end{equation}

where the regularization parameter $\tau$ controls a cutoff that keeps only singular values with indices $k\leq\tau<n$ during the inversion. Thus, higher order modes in the reconstructed distribution leading to oscillating solutions are ignored. A more sophisticated unfolding method is the Tikhonov regularization scheme~\cite{Tikhonov}. Here, the unfolding problem is rewritten as a minimization of the loss function

\begin{align}
L\big(\vec{x}\big)&=\big|\big|\,\vec{y}-\tilde{\mathcal{R}}\cdot\vec{x} \,\big|\big|^{2}~+~~\big|\big|\,\Gamma\cdot\vec{x}\,\big|\big|^{2}\,,\qquad\Rightarrow~~\frac{\partial L}{\partial x_{i}}=0\,,
\end{align}

where a suitable matrix $\Gamma$ is added to suppress unphysical solutions. In this thesis, unfolding is performed using the \TUNFOLD[] package~\cite{1748-0221-7-10-T10003} which employs a similar regularization scheme. Its loss function is given by

\begin{subequations}\label{eq:technique-tunfold}
\begin{align}
L_\mathrm{\TUNFOLD}\big(\,\vec{x},\,\lambda\,\big|~\tau\big)&=\sum_{i}\,\sum_{j}~\Big(y_{i}-\big(\tilde{\mathcal{R}}\cdot\vec{x}\,\big)_{i}\Big)\cdot\mathcal{C}_{y,ij}^{-1}\cdot\Big(y_{j}-\big(\tilde{\mathcal{R}}\cdot\vec{x}\,\big)_{j}\Big)\label{eq:technique-tunfold-response}\\
&+\tau^{2}\cdot\Big(\Gamma\cdot\big(\vec{x}-\vec{x}_{0}\big)\Big)^{2}\label{eq:technique-tunfold-regularization}\\
&+\lambda\cdot\sum_{i}~\Big(y_{i}-\mathcal{A}_{ii}\mathcal{E}_{ii}x_{i}\Big)\,. \label{eq:technique-tunfold-efficiency}
\end{align}
\end{subequations}

The matrix $\mathcal{C}$ in Eq.~\ref{eq:technique-tunfold-response} describes the statistical covariances of the bins in the reconstructed distribution. For uncorrelated data bins it is a diagonal matrix with entries $\mathcal{C}_{ii}=y_{ii}$ assuming Poisson uncertainties. The Tikhonov regularization is applied through the penalty term in Eq.~\ref{eq:technique-tunfold-regularization}. Here, solutions with large fluctuations are suppressed through the matrix $\Gamma$ which approximates numerically the second derivatives per bin of the resulting distribution as 

\begin{equation}
\frac{\mathrm{d}^{2}g(x)}{\mathrm{d}x^2}\approx\frac{g(x-h)-2\,g(x)+g(x+h)}{h^2}\,,~~\Rightarrow~\Gamma=
\begin{tikzpicture}[baseline=(current bounding box.center),every left delimiter/.style={xshift=1.1em},every right delimiter/.style={xshift=-0.3em}]
\matrix (m) [matrix of math nodes,nodes in empty cells,left delimiter=(,right delimiter={)},inner sep=0pt,nodes={inner sep=.3333em},]{
\hphantom{-}1 & -2            & \hphantom{-}1 & \hphantom{-}0 & \hphantom{-}0\\
\hphantom{-}0 & \hphantom{-}1 & -2            & \hphantom{-}1 & \hphantom{-}0  \\
              &               &               &               &   \\[0.5em]
\hphantom{-}0 & \hphantom{-}0 & \hphantom{-}1 & -2            & \hphantom{-}1 \\
} ;
\draw[dotted,thick] (m-2-2)-- +(0.65,-0.6);
\draw[dotted,thick] (m-2-1)-- +(0.65,-0.6);
\draw[dotted,thick] (m-2-3)-- +(0.65,-0.6);
\draw[dotted,thick] (m-2-4)-- +(0.65,-0.6);
\end{tikzpicture}\,.~
\end{equation}

The regularization strength is controlled via the parameter $\tau$ and needs to be optimized as detailed below. The vector $\vec{x}_{0}$ allows to bias the calculation of the second derivatives. It is usually set to the expectation from theory. This way, the regularization vanishes when unfolding the expectation from theory for closure tests since $(\Gamma\,(\vec{x}_\mathrm{exp.}-\vec{x}_{0}))^2=0$. Thus, the result is bias-free even for heavily curved expectations like e.g. $\pt$ spectra of particles. An alternative procedure to achieve this is to reweight $\vec{y}$ such that $x_{i}^\prime=x_{i}/x_{i}^\scriptn{\mathrm{exp.}}$ which lets the regularization vanish as well when unfolding the expectation since $\Gamma\,\vec{x}^{\,\prime}_\mathrm{exp.}=0$. The last part of the loss function, Eq.~\ref{eq:technique-tunfold-efficiency}, attempts to match the overall normalization of the solution by accounting for the acceptance and reconstruction efficiencies per bin. It is minimized independently with respect to the Lagrange parameter $\lambda$.

Choosing a suitable regularization strength is of crucial importance for unfolding here. If the applied regularization is too weak, the unfolding becomes unregulated and thus leads to unphysical solutions with an oscillating behavior. On the other hand, if the regularization is too strong, the solution will be biased towards the solution of minimum curvature. Here, the solution might tend towards the expectation from theory since it is always a solution with vanishing curvature when a bias vector or the described reweighting is applied. A method to analyze the unfolding behavior is to monitor the induced bin-by-bin correlations as a function of the regularization strength. The covariance matrix of the unfolded spectrum is given by propagation of uncertainty as

\begin{equation}
\mathcal{C}_{x}[\tau]=J\cdot\mathcal{C}_{y}\cdot J^{T}=\tilde{\mathcal{R}}^{-1}_\mathrm{reg.}[\tau]\cdot\mathcal{C}_{y}\cdot\big(\tilde{\mathcal{R}}^{-1}_\mathrm{reg.}[\tau]\big)^{T}\,,
\end{equation}

where $J$ denotes the Jacobian matrix which is equal to the regularized and inverted response matrix as obtained through the minimization of Eq.~\ref{eq:technique-tunfold}. This allows to calculate the averaged correlation between the unfolded bins as

\begin{equation}
\bar{\rho}_{j}[\tau]\equiv\frac{1}{n-j-1}\sum_{i}^{n-j-1}\rho_{i,i+j}[\tau]\,,\qquad \rho_{i,j}[\tau]=\frac{\mathcal{C}_{x,ij}[\tau]}{\sqrt{\mathcal{C}_{x,ii}[\tau]\cdot\mathcal{C}_{x,jj}[\tau]}}\,.\label{eq:technique-avg-correlation}
\end{equation}

Figure~\ref{fig:technique-tauscan-scan} presents the averaged correlations as a function of the regularization strength for the model defined in Eq.~\ref{eq:technique-unfolding-test-model}. This so-called ``subway plot''\footnote{Subway plots have been developed in my Master's thesis~\cite{Komm-thesis}.} shows three different regions in $\tau$ to classify the various solutions. The region on the left contains solutions where $\bar{\rho}_{1}<0$ thus indicating large anticorrelations between directly adjacent bins. These anticorrelations lead to an oscillating behavior as demonstrated in Fig.~\ref{fig:technique-ill-unfolding-true} since the regularization is chosen too weak. On the right side the averaged correlation between adjacent bins is found to be $\bar{\rho}_{1}>0$. This however indicates that the applied regularization is too strong because the solutions are biased towards a solution with minimal curvature. In the intermediate region solutions with vanishing correlations can found. However, this does not occur at the same regularization strength for all monitored correlations. Thus, there exists no regularization strength choice to achieve a correlation-free result here. In the analyses, the regularization strength is set to the minimum of the average global correlation coefficient

\begin{equation}
\rho^\mathrm{global}_{ i}[\tau]=\sqrt{1-\frac{1}{\mathcal{C}_{x,ii}[\tau]\cdot\big(\mathcal{C}_{x}^{-1}[\tau]\big)_{ii}}}\label{eq:technique-global-correlation}
\end{equation}

which results into a suitable trade-off with minimal correlation across the unfolded spectrum. Its value is indicated in Fig.~\ref{fig:technique-tauscan-scan} by the black solid curve. The resulting regularized spectrum is shown in Fig.~\ref{fig:technique-tauscan-result} which does not exhibit the oscillating behavior as shown in Fig.~\ref{fig:technique-ill-unfolding-true}. Furthermore, the regularization has stabilized the result against small deviations as indicated by the reduced variances.

\myfigure{\label{fig:technique-tauscan}Regularized unfolding using the \TUNFOLD package: (a)~the average bin-by-bin correlations~(Eqs.~\ref{eq:technique-avg-correlation} and~\ref{eq:technique-global-correlation}) as a function of the regularization strength; (b)~the resulting spectrum when choosing the regularization at the minimum of the averaged global correlation.}{
\subfloat[\label{fig:technique-tauscan-scan}]{\includegraphics[width=0.48\textwidth]{figures/technique/scanTau.pdf}}\hspace{0.03\textwidth}
\subfloat[\label{fig:technique-tauscan-result}]{\includegraphics[width=0.48\textwidth]{figures/technique/unfoldDist.pdf}}
}

In conclusion, unfolding is an ill-posed problem which requires regularization to obtain meaningful results that are stable against statistical fluctuations. The \TUNFOLD package regularizes the response matrix by suppressing solutions with large curvatures. However, this procedure induces bin-by-bin correlations as a function of the regularization strength. The correlations are kept small by adjusting the regularization strength to the minimum of the average global correlation coefficient.

